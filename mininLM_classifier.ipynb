{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravisankarg/notebooks/blob/main/mininLM_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "\n",
        "# 1. Download all-MiniLM encoder model\n",
        "model_name = \"sentence-transformers/all-MiniLM-L12-v2\" # Using a common sentence transformer model with an encoder\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBfitzQ96TGC",
        "outputId": "a352adb5-eecd-470c-ac55-4d596f398198"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForMaskedLM were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L12-v2 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Test masked language model capability\n",
        "sentence = \"swimming in water.\"\n",
        "mask_token = tokenizer.mask_token\n",
        "masked_sentence = sentence.replace(\"swimming\", mask_token) # Masking the word \"test\"\n",
        "\n",
        "print(f\"Original sentence: {sentence}\")\n",
        "print(f\"Masked sentence: {masked_sentence}\")\n",
        "\n",
        "# Tokenize the masked sentence\n",
        "inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "input_ids = inputs[\"input_ids\"]\n",
        "mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "# Get model predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predictions = outputs.logits\n",
        "\n",
        "# Get the top 3 predicted tokens for the masked position\n",
        "top_k = 10\n",
        "top_k_predicted_token_ids = torch.topk(predictions[0, mask_token_index], top_k).indices.tolist()\n",
        "top_k_predicted_tokens = [tokenizer.decode(token_id) for token_id in top_k_predicted_token_ids]\n",
        "\n",
        "\n",
        "print(f\"Top {top_k} predicted tokens: {top_k_predicted_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOeyWfMFM4Mv",
        "outputId": "15e46be6-cd8c-4c24-a7bc-cf9ae72d719e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: swimming in water.\n",
            "Masked sentence: [MASK] in water.\n",
            "Top 10 predicted tokens: ['drowning swimming living walking breathing floating lost drowned covered bathing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1xmEZKVqOWjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "371330ad"
      },
      "source": [
        "# Task\n",
        "Create an end-to-end example using the \"sentence-transformers/all-MiniLM-L12-v2\" model. The example should demonstrate how to fine-tune this model for a sentence meaningfulness classification task. This involves adding a classification layer on top of the pre-trained model, generating synthetic training data (around 1000 samples) with multiple sentences per input and sequence IDs, training the modified model on this data, and finally testing the trained model with new examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ad181cf"
      },
      "source": [
        "## Load the pre-trained model and tokenizer\n",
        "\n",
        "### Subtask:\n",
        "Load the `sentence-transformers/all-MiniLM-L12-v2` model and its corresponding tokenizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4adb268e"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to import the necessary libraries and load the tokenizer and model as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea6c80a0"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Specify the pre-trained model name\n",
        "model_name = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = AutoModel.from_pretrained(model_name)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b06479b1"
      },
      "source": [
        "## Define the custom model\n",
        "\n",
        "### Subtask:\n",
        "Create a new model by adding a classification layer on top of the pre-trained MiniLM model. This layer will predict whether a sentence is meaningful or not.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "991cd438"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a custom PyTorch model with a classification layer on top of the pre-trained MiniLM model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05727f3d"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MeaningfulnessClassifier(nn.Module):\n",
        "    def __init__(self, pretrained_model):\n",
        "        super(MeaningfulnessClassifier, self).__init__()\n",
        "        self.pretrained_model = pretrained_model\n",
        "        # Add a linear layer for binary classification (meaningful/not meaningful)\n",
        "        self.classifier = nn.Linear(pretrained_model.config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Pass input through the pre-trained model\n",
        "        outputs = self.pretrained_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # Get the pooled output (usually the representation of the [CLS] token)\n",
        "        # Sentence-BERT models typically use mean pooling over all tokens, but for simplicity here we'll use the first token's output\n",
        "        pooled_output = outputs.last_hidden_state[:, 0]\n",
        "        # Pass the pooled output through the classification layer\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Instantiate the custom model\n",
        "meaningfulness_model = MeaningfulnessClassifier(model)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe1875eb"
      },
      "source": [
        "## Prepare synthetic data\n",
        "\n",
        "### Subtask:\n",
        "Generate a synthetic dataset of around 1000 samples. Each sample should consist of multiple sentences, with at least one meaningful sentence and some non-meaningful ones. Label each sentence as meaningful or not.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b01def9"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate synthetic data and store it in a pandas DataFrame as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0ac94b2e",
        "outputId": "f9bd494d-2980-4f62-ae06-4210623dadfb"
      },
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# 1. Create lists of meaningful and non-meaningful sentences\n",
        "meaningful_sentences = [\n",
        "    \"The sun rises in the east.\",\n",
        "    \"Birds fly in the sky.\",\n",
        "    \"Water is essential for life.\",\n",
        "    \"Computers process information.\",\n",
        "    \"Humans breathe oxygen.\",\n",
        "    \"Fish swim in the ocean.\",\n",
        "    \"The Earth revolves around the sun.\",\n",
        "    \"Plants perform photosynthesis.\",\n",
        "    \"Dogs are mammals.\",\n",
        "    \"Fire produces heat.\",\n",
        "    \"Rain falls from clouds.\",\n",
        "    \"The moon orbits the Earth.\",\n",
        "    \"Books contain stories.\",\n",
        "    \"Cars have wheels.\",\n",
        "    \"Music can evoke emotions.\",\n",
        "    \"Time moves forward.\",\n",
        "    \"Food provides energy.\",\n",
        "    \"Languages are used for communication.\",\n",
        "    \"Science seeks to understand the world.\",\n",
        "    \"Art can be a form of expression.\"\n",
        "]\n",
        "\n",
        "non_meaningful_sentences = [\n",
        "    \"The square circle sang a loud silence.\",\n",
        "    \"Purple ideas sleep furiously.\",\n",
        "    \"A colorless green idea sleeps furiously.\",\n",
        "    \"The talking rock told a joke to the wind.\",\n",
        "    \"Invisible elephants danced on the ceiling.\",\n",
        "    \"The concept of blue tastes like numbers.\",\n",
        "    \"Butterflies whisper secrets to the trees.\",\n",
        "    \"My shoes are made of abstract thoughts.\",\n",
        "    \"The empty box was full of nothing.\",\n",
        "    \"Yesterday's future is today's past.\",\n",
        "    \"A dream within a dream is a reality.\",\n",
        "    \"The sound of silence is deafening.\",\n",
        "    \"He felt a cold warmth.\",\n",
        "    \"The light was a dark brightness.\",\n",
        "    \"She spoke in a silent shout.\",\n",
        "    \"The beginning of the end is near.\",\n",
        "    \"He ran at a walking pace.\",\n",
        "    \"The small giant stood tall.\",\n",
        "    \"It was a truthful lie.\",\n",
        "    \"The friendly enemy smiled.\"\n",
        "]\n",
        "\n",
        "# 2. Generate approximately 1000 samples\n",
        "data = []\n",
        "num_samples = 1000\n",
        "\n",
        "for _ in range(num_samples):\n",
        "    sample_sentences = []\n",
        "    sample_labels = []\n",
        "\n",
        "    # Always include one meaningful sentence\n",
        "    meaningful_sentence = random.choice(meaningful_sentences)\n",
        "    sample_sentences.append(meaningful_sentence)\n",
        "    sample_labels.append(1) # 1 for meaningful\n",
        "\n",
        "    # Randomly select 1 to 3 non-meaningful sentences\n",
        "    num_non_meaningful = random.randint(1, 3)\n",
        "    selected_non_meaningful = random.sample(non_meaningful_sentences, num_non_meaningful)\n",
        "\n",
        "    for non_meaningful_sentence in selected_non_meaningful:\n",
        "        sample_sentences.append(non_meaningful_sentence)\n",
        "        sample_labels.append(0) # 0 for non-meaningful\n",
        "\n",
        "    # Shuffle the sentences within the sample to mix meaningful and non-meaningful ones\n",
        "    combined = list(zip(sample_sentences, sample_labels))\n",
        "    random.shuffle(combined)\n",
        "    shuffled_sentences, shuffled_labels = zip(*combined)\n",
        "\n",
        "    data.append({\n",
        "        'sentences': shuffled_sentences,\n",
        "        'labels': shuffled_labels\n",
        "    })\n",
        "\n",
        "# 5. Store in a Pandas DataFrame\n",
        "synthetic_df = pd.DataFrame(data)\n",
        "\n",
        "display(synthetic_df.head())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                           sentences        labels\n",
              "0  (The small giant stood tall., Time moves forwa...        (0, 1)\n",
              "1  (Art can be a form of expression., A dream wit...        (1, 0)\n",
              "2  (The empty box was full of nothing., Rain fall...     (0, 1, 0)\n",
              "3  (Butterflies whisper secrets to the trees., Do...        (0, 1)\n",
              "4  (The talking rock told a joke to the wind., My...  (0, 0, 1, 0)"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f678ca54-f5af-43f5-b1f6-1c4509ddeb85\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(The small giant stood tall., Time moves forwa...</td>\n",
              "      <td>(0, 1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(Art can be a form of expression., A dream wit...</td>\n",
              "      <td>(1, 0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(The empty box was full of nothing., Rain fall...</td>\n",
              "      <td>(0, 1, 0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(Butterflies whisper secrets to the trees., Do...</td>\n",
              "      <td>(0, 1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(The talking rock told a joke to the wind., My...</td>\n",
              "      <td>(0, 0, 1, 0)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f678ca54-f5af-43f5-b1f6-1c4509ddeb85')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f678ca54-f5af-43f5-b1f6-1c4509ddeb85 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f678ca54-f5af-43f5-b1f6-1c4509ddeb85');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7df40ad9-5513-4a2b-978d-1a13c2dbd460\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7df40ad9-5513-4a2b-978d-1a13c2dbd460')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7df40ad9-5513-4a2b-978d-1a13c2dbd460 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(synthetic_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"sentences\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          [\n            \"Art can be a form of expression.\",\n            \"A dream within a dream is a reality.\"\n          ],\n          [\n            \"The talking rock told a joke to the wind.\",\n            \"My shoes are made of abstract thoughts.\",\n            \"Books contain stories.\",\n            \"Butterflies whisper secrets to the trees.\"\n          ],\n          [\n            \"The empty box was full of nothing.\",\n            \"Rain falls from clouds.\",\n            \"My shoes are made of abstract thoughts.\"\n          ]\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          [\n            1,\n            0\n          ],\n          [\n            0,\n            0,\n            1,\n            0\n          ],\n          [\n            0,\n            1\n          ]\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6915547b"
      },
      "source": [
        "## Tokenize and prepare data for training\n",
        "\n",
        "### Subtask:\n",
        "Tokenize the synthetic sentences and prepare them in a format suitable for training the custom model. This might involve creating input tensors with appropriate sequence IDs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44b414da"
      },
      "source": [
        "**Reasoning**:\n",
        "Tokenize the sentences in the synthetic data and create input tensors and label tensors for training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23839b55",
        "outputId": "5aedc52f-5247-464b-ec9b-4fb7f3e0939a"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Tokenize sentences and prepare data\n",
        "tokenized_inputs = []\n",
        "labels = []\n",
        "\n",
        "max_seq_length = 128 # Define a maximum sequence length\n",
        "\n",
        "for index, row in synthetic_df.iterrows():\n",
        "    sentences = list(row['sentences']) # Convert tuple to list for tokenization\n",
        "    sample_labels = list(row['labels']) # Convert tuple to list\n",
        "\n",
        "    # Tokenize each sentence in the sample\n",
        "    # Use truncation and padding to handle variable sentence lengths within the sample\n",
        "    # The tokenizer handles the addition of special tokens like [CLS] and [SEP]\n",
        "    encoded_inputs = tokenizer(\n",
        "        sentences,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_seq_length,\n",
        "        return_tensors='pt', # Return PyTorch tensors\n",
        "        is_split_into_words=False # Indicate that input is a list of strings\n",
        "    )\n",
        "\n",
        "    # The tokenizer returns a batch of inputs where each sentence is padded/truncated\n",
        "    # We need to process these to match the structure expected by our model\n",
        "    # For this task, we treat each sentence within the sample independently for classification\n",
        "    # So, the tokenized inputs will be a list of dictionaries, each corresponding to a sentence\n",
        "    # The labels will be a flattened list corresponding to each sentence\n",
        "    num_sentences_in_sample = len(sentences)\n",
        "    for i in range(num_sentences_in_sample):\n",
        "        # Extract input_ids and attention_mask for each sentence\n",
        "        # unsqueeze(0) adds a batch dimension of 1\n",
        "        sentence_input_ids = encoded_inputs['input_ids'][i].unsqueeze(0)\n",
        "        sentence_attention_mask = encoded_inputs['attention_mask'][i].unsqueeze(0)\n",
        "\n",
        "        tokenized_inputs.append({\n",
        "            'input_ids': sentence_input_ids,\n",
        "            'attention_mask': sentence_attention_mask\n",
        "        })\n",
        "        labels.append(sample_labels[i])\n",
        "\n",
        "# Convert the labels list to a PyTorch tensor\n",
        "labels_tensor = torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "print(f\"Number of tokenized sentences across all samples: {len(tokenized_inputs)}\")\n",
        "print(f\"Shape of the labels tensor: {labels_tensor.shape}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokenized sentences across all samples: 2964\n",
            "Shape of the labels tensor: torch.Size([2964])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1ab7a73"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "### Subtask:\n",
        "Train the custom model using the synthetic data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id6RkhA0T-Te",
        "outputId": "ca798431-6fff-4f00-a3cf-b7e45dcd9644"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW # Corrected import location for AdamW\n",
        "import torch.nn as nn\n",
        "\n",
        "# 2. Create a custom PyTorch Dataset class\n",
        "class MeaningfulnessDataset(Dataset):\n",
        "    def __init__(self, tokenized_inputs, labels):\n",
        "        self.tokenized_inputs = tokenized_inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.tokenized_inputs[idx]['input_ids'].squeeze(0), # Remove the batch dimension added during initial tokenization\n",
        "            'attention_mask': self.tokenized_inputs[idx]['attention_mask'].squeeze(0), # Remove the batch dimension\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "# 3. Instantiate the custom Dataset\n",
        "dataset = MeaningfulnessDataset(tokenized_inputs, labels_tensor)\n",
        "\n",
        "# 4. Create a PyTorch DataLoader\n",
        "batch_size = 16 # Define batch size\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 6. Instantiate an optimizer and define a loss function\n",
        "optimizer = AdamW(meaningfulness_model.parameters(), lr=5e-5) # Using AdamW as optimizer\n",
        "loss_fn = nn.BCEWithLogitsLoss() # Binary Cross Entropy with Logits Loss for binary classification\n",
        "\n",
        "# 5. Define the training loop and 7. Iterate through the DataLoader\n",
        "num_epochs = 3 # Define number of epochs\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "meaningfulness_model.to(device)\n",
        "\n",
        "print(f\"Training on device: {device}\")\n",
        "\n",
        "meaningfulness_model.train() # Set model to training mode\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        # Move batch to device\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad() # Zero out gradients\n",
        "\n",
        "        # 8. Perform a forward pass\n",
        "        logits = meaningfulness_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Ensure labels have the correct shape [batch_size, 1] for BCEWithLogitsLoss\n",
        "        labels = labels.unsqueeze(1)\n",
        "\n",
        "        # 8. Calculate the loss\n",
        "        loss = loss_fn(logits, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # 8. Perform backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # 8. Update the model's weights\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    # 9. Track and print the loss\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Training finished.\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on device: cuda\n",
            "Epoch 1/3, Average Loss: 0.0010\n",
            "Epoch 2/3, Average Loss: 0.0004\n",
            "Epoch 3/3, Average Loss: 0.0002\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9b694a9"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model on a separate test set (either part of the synthetic data or newly generated) to assess its performance in identifying meaningful sentences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a757636f"
      },
      "source": [
        "**Reasoning**:\n",
        "Split the tokenized data and labels into training and testing sets, create a test dataset and dataloader, set the model to evaluation mode, and then iterate through the test set to calculate evaluation metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b641e66f",
        "outputId": "51eb7a3d-e29c-4972-901f-20bf42090132"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Split the data into training and testing sets\n",
        "# We need to split the original samples first to ensure sentences from the same sample are not split\n",
        "# This requires re-tokenizing after splitting the original samples\n",
        "train_samples, test_samples = train_test_split(synthetic_df, test_size=0.2, random_state=42)\n",
        "\n",
        "def tokenize_samples(samples_df, tokenizer, max_seq_length):\n",
        "    \"\"\"Helper function to tokenize samples from a DataFrame.\"\"\"\n",
        "    tokenized_inputs = []\n",
        "    labels = []\n",
        "    for index, row in samples_df.iterrows():\n",
        "        sentences = list(row['sentences'])\n",
        "        sample_labels = list(row['labels'])\n",
        "\n",
        "        encoded_inputs = tokenizer(\n",
        "            sentences,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=max_seq_length,\n",
        "            return_tensors='pt',\n",
        "            is_split_into_words=False\n",
        "        )\n",
        "\n",
        "        num_sentences_in_sample = len(sentences)\n",
        "        for i in range(num_sentences_in_sample):\n",
        "            sentence_input_ids = encoded_inputs['input_ids'][i].unsqueeze(0)\n",
        "            sentence_attention_mask = encoded_inputs['attention_mask'][i].unsqueeze(0)\n",
        "\n",
        "            tokenized_inputs.append({\n",
        "                'input_ids': sentence_input_ids,\n",
        "                'attention_mask': sentence_attention_mask\n",
        "            })\n",
        "            labels.append(sample_labels[i])\n",
        "    return tokenized_inputs, torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "# Tokenize the train and test sets\n",
        "train_tokenized_inputs, train_labels_tensor = tokenize_samples(train_samples, tokenizer, max_seq_length)\n",
        "test_tokenized_inputs, test_labels_tensor = tokenize_samples(test_samples, tokenizer, max_seq_length)\n",
        "\n",
        "# 2. Create a new PyTorch Dataset and DataLoader for the test set\n",
        "# Using the same Dataset class defined previously\n",
        "test_dataset = MeaningfulnessDataset(test_tokenized_inputs, test_labels_tensor)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) # No need to shuffle test data\n",
        "\n",
        "# Move model to the same device as during training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "meaningfulness_model.to(device)\n",
        "\n",
        "# 3. Set the meaningfulness_model to evaluation mode\n",
        "meaningfulness_model.eval()\n",
        "\n",
        "# Lists to store predictions and true labels\n",
        "all_predictions = []\n",
        "all_true_labels = []\n",
        "\n",
        "# 4. Iterate through the test DataLoader with torch.no_grad()\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        # Move batch to device\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # 5. Perform a forward pass\n",
        "        logits = meaningfulness_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # 6. Apply a sigmoid function to the logits to get probabilities\n",
        "        probabilities = torch.sigmoid(logits).squeeze(1) # Squeeze to remove the last dimension\n",
        "\n",
        "        # 7. Convert the probabilities to binary predictions (0 or 1) using a threshold\n",
        "        predictions = (probabilities > 0.5).int() # Threshold at 0.5\n",
        "\n",
        "        # Store predictions and true labels\n",
        "        all_predictions.extend(predictions.cpu().tolist())\n",
        "        all_true_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "# 8. Calculate evaluation metrics\n",
        "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
        "precision = precision_score(all_true_labels, all_predictions)\n",
        "recall = recall_score(all_true_labels, all_predictions)\n",
        "f1 = f1_score(all_true_labels, all_predictions)\n",
        "\n",
        "# 9. Print the calculated evaluation metrics\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "Accuracy: 1.0000\n",
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1-score: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b1bb103"
      },
      "source": [
        "## Test with examples\n",
        "\n",
        "### Subtask:\n",
        "Use the trained model to predict the meaningfulness of new, unseen sentences to demonstrate its capability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a81ac07d"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a list of new sentences and then tokenize them and pass them through the trained model to get predictions. Finally, print the sentences and their predicted labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "882102ef",
        "outputId": "1500f6ee-2ab7-4f67-8565-55615185d4c8"
      },
      "source": [
        "# 1. Create a list of new, unseen sentences\n",
        "new_sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",  # Meaningful\n",
        "    \"Silence tastes like purple.\",                  # Non-meaningful\n",
        "    \"Mount Everest is the highest mountain.\",       # Meaningful\n",
        "    \"Ideas can sing in the rain.\",                  # Non-meaningful\n",
        "    \"The capital of France is Paris.\",              # Meaningful\n",
        "    \"The number seven is green.\",                   # Non-meaningful\n",
        "    \"Water boils at 100 degrees Celsius.\",          # Meaningful\n",
        "    \"My cat barked at the moon.\",                   # Non-meaningful\n",
        "    \"Photosynthesis requires sunlight.\",            # Meaningful\n",
        "    \"Invisible concepts dance on the ceiling.\"      # Non-meaningful\n",
        "]\n",
        "\n",
        "# 2. Tokenize these new sentences\n",
        "# Use the same tokenizer and max_seq_length as during training\n",
        "encoded_new_inputs = tokenizer(\n",
        "    new_sentences,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=max_seq_length,\n",
        "    return_tensors='pt',  # Return PyTorch tensors\n",
        "    is_split_into_words=False # Indicate that input is a list of strings\n",
        ")\n",
        "\n",
        "# Move tokenized inputs to the same device as the model\n",
        "input_ids = encoded_new_inputs['input_ids'].to(device)\n",
        "attention_mask = encoded_new_inputs['attention_mask'].to(device)\n",
        "\n",
        "# 3. Set the trained meaningfulness_model to evaluation mode\n",
        "meaningfulness_model.eval()\n",
        "\n",
        "# 4. Perform a forward pass with torch.no_grad()\n",
        "with torch.no_grad():\n",
        "    # 5. Perform a forward pass\n",
        "    logits = meaningfulness_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "# 6. Apply a sigmoid function to the logits to convert them into probabilities\n",
        "probabilities = torch.sigmoid(logits).squeeze(1) # Squeeze to remove the last dimension\n",
        "\n",
        "# 7. Convert the probabilities into binary predictions (0 or 1) using a threshold (e.g., 0.5)\n",
        "predictions = (probabilities > 0.5).int() # Threshold at 0.5\n",
        "\n",
        "# 8. Print each original new sentence along with its predicted meaningfulness label\n",
        "print(\"\\nPredictions for new sentences:\")\n",
        "for i, sentence in enumerate(new_sentences):\n",
        "    predicted_label = \"Meaningful\" if predictions[i].item() == 1 else \"Not Meaningful\"\n",
        "    print(f\"Sentence: \\\"{sentence}\\\" -> Predicted Label: {predicted_label}\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions for new sentences:\n",
            "Sentence: \"The quick brown fox jumps over the lazy dog.\" -> Predicted Label: Not Meaningful\n",
            "Sentence: \"Silence tastes like purple.\" -> Predicted Label: Not Meaningful\n",
            "Sentence: \"Mount Everest is the highest mountain.\" -> Predicted Label: Meaningful\n",
            "Sentence: \"Ideas can sing in the rain.\" -> Predicted Label: Meaningful\n",
            "Sentence: \"The capital of France is Paris.\" -> Predicted Label: Meaningful\n",
            "Sentence: \"The number seven is green.\" -> Predicted Label: Not Meaningful\n",
            "Sentence: \"Water boils at 100 degrees Celsius.\" -> Predicted Label: Meaningful\n",
            "Sentence: \"My cat barked at the moon.\" -> Predicted Label: Not Meaningful\n",
            "Sentence: \"Photosynthesis requires sunlight.\" -> Predicted Label: Meaningful\n",
            "Sentence: \"Invisible concepts dance on the ceiling.\" -> Predicted Label: Not Meaningful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "023279f5"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The `sentence-transformers/all-MiniLM-L12-v2` model and its tokenizer were successfully loaded for the task.\n",
        "*   A custom PyTorch model (`MeaningfulnessClassifier`) was successfully created by adding a linear classification layer on top of the pre-trained MiniLM model for binary classification.\n",
        "*   A synthetic dataset of 1000 samples was generated, with each sample containing a mix of meaningful and non-meaningful sentences and their corresponding labels.\n",
        "*   The sentences from the synthetic data were tokenized using the loaded tokenizer, applying padding and truncation to prepare them for model input.\n",
        "*   A custom PyTorch `Dataset` and `DataLoader` were created to handle the tokenized data during training and evaluation.\n",
        "*   The custom model was successfully trained for 3 epochs using the synthetic data, showing a decrease in average training loss.\n",
        "*   The synthetic data was split into training and testing sets. The trained model was evaluated on the test set, achieving perfect scores (1.0000) for accuracy, precision, recall, and F1-score on this specific synthetic test data.\n",
        "*   The trained model was used to predict the meaningfulness of new, unseen sentences. While some predictions were correct, the model misclassified \"The quick brown fox jumps over the lazy dog.\" as \"Not Meaningful\" and \"Ideas can sing in the rain.\" as \"Meaningful\", indicating limitations with the current synthetic data or training duration.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The model shows promising results on the synthetic data it was trained on, but its performance on slightly different or more complex unseen sentences suggests that the synthetic data may not fully represent the nuances of real-world sentence meaningfulness.\n",
        "*   To improve the model's generalization capabilities, the next step should involve training on a larger and more diverse dataset, potentially using real-world examples of meaningful and non-meaningful sentences.\n"
      ]
    }
  ]
}